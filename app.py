import streamlit as st
import cv2
import av
from streamlit_webrtc import webrtc_streamer
from streamlit_mic_recorder import speech_to_text
from codecarbon import EmissionsTracker
import time

# Ø¥Ø¹Ø¯Ø§Ø¯ ØªØªØ¨Ø¹ Ø§Ù†Ø¨Ø¹Ø§Ø«Ø§Øª Ø§Ù„ÙƒØ´Ùƒ
tracker = EmissionsTracker(save_to_file=False)
tracker.start()

st.set_page_config(page_title="AI Climate Pod", layout="centered")

st.title("ğŸŒ ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ù…Ù†Ø§Ø®ÙŠ (Replit Edition)")

# 1. Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬Ù‡
st.header("ğŸ‘¤ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø§Ø¦Ø±")
def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    for (x, y, w, h) in faces:
        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 3)
    return av.VideoFrame.from_ndarray(img, format="bgr24")

webrtc_streamer(key="face", video_frame_callback=video_frame_callback)

# 2. Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØµÙˆØªÙŠØ©
st.header("ğŸ¤ Ø³Ø¬Ù„ Ø¨ØµÙ…ØªÙƒ Ø§Ù„ØµÙˆØªÙŠØ©")
text = speech_to_text(language='ar', start_prompt="ØªØ­Ø¯Ø« Ø§Ù„Ø¢Ù†...", key='speech')
if text:
    st.success(f"Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø³Ù…Ø¹: {text}")

# 3. Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø«Ø± Ø§Ù„Ø¨ÙŠØ¦ÙŠ
if st.button("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"):
    emissions = tracker.stop()
    st.metric("Ø¨ØµÙ…Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ±Ø¨ÙˆÙ†ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…", f"{emissions:.6f} kg CO2")
    st.balloons()
